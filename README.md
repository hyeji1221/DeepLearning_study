# DeepLearning_study

|          개념          |                             설명                             |
| :--------------------: | :----------------------------------------------------------: |
|       선형 회귀        |    선형 방정식을 사용하여 연속적인 값을 예측하는 알고리즘    |
|       경사하강법       | 모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)을 사용하여 <br />모델을 조금씩 조정하는 최적화 알고리즘 <br />-> 오차를 줄이는 방향으로 가중치를 조정한다 |
|       손실 함수        | 모델이 얼마만큼의 오류가 있는지 측정하는 기준 <br />-> 이 함수의 값을 최소로 만드는 것이 모델 훈련의 목적 |
|     로지스틱 회귀      | 이진 분류가 목표<br />사용하는 활성화 함수는 **시그모이드 함수**로 z를 0~1 사이의 확률값으로 변환시켜준다. <br />p = 1/1+e^-z |
|   로지스틱 손실함수    | 다중분류를 위한 손실 함수인 크로스 엔트로피 손실 함수의 이진 분류 버전<br />-> L = -(ylog(a) + (1-y)log(1-a)<br />미분을 통해 로지스틱 손실 함수의 값을 최소로 하는 가중치와 절편을 찾아야 한다. <br />-> 제곱 오차를 미분한 결과와 동일 (가중치 : -(y-a)x, 절편 : -(y-a)1) |
|    아담 옵티마이저     | 손실 함수의 값이 최적값에 가까워질수록 학습률을 낮춰 손실 함수의 값이 안정적으로 수렴될 수 있게 한다 |
|       드롭 아웃        | 신경망에서 과대적합을 줄이는 방법 중 하나<br /> 무작위로 뉴런을 비활성화 시킨다 -> 특정 뉴런에 과도하게 의존하여 훈련하는 것을 막는다 |
|       렐루 함수        | 합성곱 신경망에서 자주 사용하는 활성화 함수<br /> 0보다 큰 값은 그대로 통과시키고 0보다 작은 값은 0으로 만든다 |
| 하이퍼볼릭<br />탄젠트 |       순환 신경망의 셀에서 사용하는 활성화함수 (tanh)        |
