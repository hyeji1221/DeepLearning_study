
## 7장 여러 개를 분류하는 다중분류
### 소프트맥스 함수 
- 출력층의 출력 강도를 정규화 -> 전체 출력값의 합을 1로 만든다 -> 값들을 확률로 생각 가능 

- 출력이 늘어나는 만큼 지수 함수가 늘어난다

- 다중 분류에서는 출력층에 항상 소프트맥스 함수를 사용한다 & 이진 분류의 경우에는 항상 시그모이드 함수 사용한다

### 다중분류에서 사용하는 손실함수 
- 로지스틱 손실 함수의 일반화 버전인 크로스 엔트로피 손실 함수

- L = log(a) -> 여기서 a는 y가1인 소프트맥스 함수의 출력값 (정답 클래스에 해당하는 뉴런의 활성화 출력)

- 미분한 결과는 -(y-a)로 타깃과 소프트맥스의 출력값을 뺀 값을 오차로 역전파 하면 된다. -> 로지스틱 손실 함수의 미분과 정확히 일치

### 다중 분류 신경망을 구현 (209p)
지금까지는 활성화 함수로 시그모이드 함수만 출력 하였기에 마지막 출력 함수에서 소프트맥스 함수를 사용해야 한다 -> activation() 메서드의 이름을 sigmoid()로 바꾸고 softmax() 메서드를 추가한다.